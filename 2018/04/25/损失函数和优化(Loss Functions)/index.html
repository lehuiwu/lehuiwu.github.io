<!DOCTYPE HTML>
<html lang="zh-CN">
<head>
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="Be kind,Be useful">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <link rel="dns-prefetch" href="http://yoursite.com">
    <!--SEO-->





<meta name="robots" content="all" />
<meta name="google" content="all" />
<meta name="googlebot" content="all" />
<meta name="verify" content="all" />
    <!--Title-->


<title>损失函数和优化（Loss Functions） | Be kind,Be useful</title>


    <link rel="alternate" href="/atom.xml" title="Be kind,Be useful" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    


    <link rel="stylesheet" href="//imsun.github.io/gitment/style/default.css">


<link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.4/css/bootstrap.min.css?rev=3.3.4">
<link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
		<script type="text/javascript">
			var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
		</script>
	</div>






    
</head>


<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <header class="main-header"  style="background-image:url(http://7xpw2b.com1.z0.glb.clouddn.com/hexo-sinppet/img/banner2.jpg)"  >
    <div class="main-header-box">
        <a class="header-avatar" href="/" title='PreCon'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
        	<!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
                <h2> 不忘初心 方得始终 </h2>
            
    	</div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>首页</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/深度学习/"><i class="fa "></i>深度学习</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/随笔/"><i class="fa "></i>随笔</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/工具/"><i class="fa "></i>工具</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/资源/"><i class="fa "></i>资源</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="损失函数和优化（Loss Functions）">
            
	            损失函数和优化（Loss Functions）
            
        </h1>
        <div class="post-meta">
    
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a href="/categories/深度学习">
            深度学习
        </a>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
                
                    <a href="/tags/cs231n" title='cs231n'>
                        cs231n
                    </a>
                
            
        </span>
    </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2018/04/25</span>
        </span>
    
</div>

            
            
    </div>
    
    <div class="post-body post-content">
        <p><strong>本内容为学习斯坦福课程CS231n 2017的课后笔记记录</strong><br><a id="more"></a><br><strong>斯坦福cs231n课程笔记 <em>损失函数</em></strong>  </p>
<p><div align="center"><img src="https://i.imgur.com/LQ2zKQ3.png" alt=""><div align="left">  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/2aUuyfY.png" alt=""><div align="left">  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/WxLrcfP.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/4MmxrBA.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/Up1Ilch.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/sn3i4cC.png" alt=""><div align="left"><br>&emsp;&emsp;S:是通过分类器预测出来的类的分数。Si:指为对应i类的分数。Syi:代表了训练集的第i个样本的真实分类的分数。  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/vy9rkwq.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/QnVuchs.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/P9SgR96.png" alt=""><div align="left"><br>&emsp;&emsp;<strong>为啥选择加上参数1：</strong><br>&emsp;&emsp;这其实在一定程度上是一个任意的选择，这实际上就是一个出现在损失函数中的常数，但实际上证明这是一个可以任意选择的值，因为我们并不真正关心损失函数中分数的绝对值，我们只担心这些分数的相对差值，我们只需要正确分类的分数，要远远大于不正确分类的分数，所以实际上如果你把你的整个W参数，放大或者缩小，那么所有的分数都会相应地放大或者缩小。这里选择1并不重要，这个自由参数1会消失，在这种放缩过程中消失，就像对于整个参数W的放缩一样。  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/HwTPr3e.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/x7KoSN0.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/9RbhqKy.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/XPypKDj.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/DiPXzHM.png" alt=""><div align="left"><br><strong>Q1:如果汽车分数改变了，损失函数变化如何？</strong><br>&emsp;&emsp;答案就是如果汽车图像的分数发生了轻微的变化，那么损失函数将不会变化，SVM损失函数，记住，只关注于正确的分数比不正确的分数大过1，但是在这种情况下，汽车的分数比其他的都要大，所以如果汽车的分数改变了，只是稍微改变了一丢丢，那么1的界限依然奏效，损失函数并不会改变，我们依然得到为0的损失函数。</div></div></p>
<p><div align="center"><img src="https://i.imgur.com/I32HAO7.png" alt=""><div align="left"><br><strong>Q2:损失函数的最大值和最小值会是多少？</strong><br>&emsp;&emsp;答案：损失函数的最大值为无穷大，最小值为零。</div></div></p>
<p><div align="center"><img src="https://i.imgur.com/vKk0Y3P.png" alt=""><div align="left"><br><strong>Q3:当你初始化这些参数（W）并且从头开始训练，通常你先使用一些很小的随机值来，初始化W，你的分数的结果，在训练的初期倾向于呈现较小的均匀分布的值，并且问题在于如果你所有的S，也就是你所有的分数都近乎为0，并且差不多相等，那么当你使用多分类SVM时，损失函数预计会是如何？</strong><br>&emsp;&emsp;答案：分类的数量减去1。因为如果我们对所有不正确的类别遍历了一遍，那么实际上我们遍历了C-1个类别，在这些类别中的每一个，这两个分数差不多相同，所以我们就会得到一个值为1的损失项，因为存在着1的边界，我们将会得到C-1，这个结论实际上是有用的因为，这是一个有用的调试策略，当你使用这些方法的时候，当你开始训练的时候，你应该想到你的预期的损失函数应该是多达，如果在刚开始训练的时候你的损失函数，在第一次迭代的时候损失函数并不等于C-1，如果这样的话，着意味着你的程序可能有一个BUG，你得去检查一下，这是一个有用的结论。</div></div></p>
<p><div align="center"><img src="https://i.imgur.com/PWFPOF3.png" alt=""><div align="left"><br><strong>Q4：如果我们将对于SVM的所有错误的分数求和会发生什么，如果我们将所有正确的分数求和会发生什么？</strong><br>&emsp;&emsp;答案：损失函数增加1。同时我们在实际应用中这么做的原因在于，通常损失函数为0的时候说明算法很好，所以你没有损失什么，这就很好了，所以我觉得你们的答案不会改变，你就不会再去寻求同样的分类器了，如果实际上你求和遍历了所有的类别，但是如果我们疏忽了正确的类别，那么我们的最小损失函数为0。  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/mhhaCqy.png" alt=""><div align="left"><br><strong>Q5:如果我们使用平均值，而不是求和呢？</strong><br>&emsp;&emsp;答案：不会改变，所以分类的数量需要提前确定，当我们选择数据集的时候，因为这只是将整个损失函数缩小了一个倍数，所以这并没有什么影响，任何缩放的操作，都不会有什么影响，因为我们实际上并不在意真正的分数值，或者是损失函数的真实值。  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/eOEBT4j.png" alt=""><div align="left"><br><strong>Q6:如果我们改变损失函数的公式，在max上加上一个平方项呢？这会成为另一个不同的分类算法么？</strong><br>&emsp;&emsp;答案：是不同的。所以这里的想法在于我们用一种非线性的方法，改变了在好和坏之间的权衡，所以实际上我们计算了另一种损失函数，关于合页损失函数的平方项的想法有时候，确实会在实际中应用，这是另一个技巧，当你针对你自己的问题，构成你自己的损失函数的时候。</div></div></p>
<p><strong>Q7:为啥用平方项损失函数而不是非平方项的损失函数？</strong><br>&emsp;&emsp;答案：问题在于你考虑使用一个平方项损失函数而不是非平方项的损失函数，一个损失函数的全部意义在于量化不同的错误到底有多坏，同时分类器会犯不同的错误，我们如何对分类器可能犯的不同类型的错误进行权衡，如果你使用平方项损失函数，这意味着一个非常非常不好的错误，会更加不好，成平方地不好，那就真的很不好，我们并不想要任何被严重分错的结果，如果你使用合页损失函数，我们对于微小的错误并不在意，但是如果分类出现很多错误，出现很多错误，如果一个样例中出现了很多的错误，那么我们就扩大错误，以此来减小这个错误，这样就像对于仅仅有微小错误的例子，扩大错误，以此来纠正，所以这是一个优点波浪，但是这个使用线性函数与平方地思想，是量化我们关心多少的一种方法，关于不同类别的错误。</p>
<p><div align="center"><img src="https://i.imgur.com/Jnql7VE.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/jAHbSGn.png" alt=""><div align="left"><br><strong>E.g. Suppose that we found a W such that L = 0. Is this W unique ?</strong><br>&emsp;&emsp;答案：No! 2W is also has L=0!，特别是因为我们谈了一点点，关于把整个问题扩大或缩小的事情，取决于W，你可以拿W乘以二，这个两倍的W，也将实现零损失。如果你拿W 我们加倍W，正确与不正确的安全边际值，也会翻倍，所以如果所有这些安全边际，都大于1，结果我们就对其翻倍，其值也会大于1，结果损失函数值依然为0。<br>&emsp;&emsp;例子如下图所示：  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/MzPcTxB.png" alt=""><div align="left"></div></div></p>
<h2 id="正则项惩罚"><a href="#正则项惩罚" class="headerlink" title="正则项惩罚"></a>正则项惩罚</h2><p><div align="center"><img src="https://i.imgur.com/6efsk2c.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/4wdAGJ5.png" alt=""><div align="left"><br><strong>E.g.分类器是如何从这些都是0值得损失函数之间，做出选择呢？</strong><br>&emsp;&emsp;那是因为我们在这里所做的，只是在数据方面的损失，我们仅仅是告诉分类器，他需要尝试找到，可以拟合训练集的W，但是实际上，我们并不关心这很多关于拟合训练数据，机器学习的重点是我们使用训练数据来找到一些分类器，然后我们将这个东西应用于测试数据，所以我们并不关心训练集的表现，我们真正关心的是这个分类器的测试数据的效果，所以如果我们只需要告诉分类器，拟合训练集的话，就可能导致我们陷入尴尬的境地，你会发现，分类器可能会行为反常，所以这是一个典型的例子。  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/fwAO4xE.png" alt=""><div align="left"><br><strong>假设我们有这样的数据，就是图中的蓝点，并且我们将会为训练数据拟合一些曲线，那么如果我们告诉分类器做的惟一的事情，是尝试和适合的训练数据，它可能会进入并具有非常曲折的曲线，并尝试完美分类所有的训练数据点，但这很糟糕，因为我们实际上并不关心这个表现，我们关心测试数据的性能，所以现在如果我们有新的数据进来，这种趋势也是如此，那么这个蓝色的虚线将是完全错误的，事实上，我们可能会喜欢的是，分类器可能是预测的这条绿色的直线，而不是这个复杂的蓝色的曲线，完全适合所有的训练数据，这在机器学习里是一个非常核心的基础性问题，实际上我们解决它用正则化的概念。</strong>  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/3N9KZan.png" alt=""><div align="left"></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/B01PzS5.png" alt=""><div align="left"><br>&emsp;&emsp;<strong>正则化项，鼓励模型以某种方式，选择更简单的W，这里的简单取决于任务的规模和模型的种类。这里实际上也体现了奥卡姆剃刀的理念，也是科学发现最基本的思想就是要让一个理论的应用更广泛，也就是说，如果你有许多个可以解释你观察结果的假设，一般来讲你应该选择最简约的，因为这样在未来可将其用于解释新的观察结果，我们运用这种直觉的方式，基于这一思想并运用于机器学习中，我们会直接假设正则化惩罚项，这通常记为R。</strong></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/TqVGoos.png" alt=""><div align="left"><br><strong>Q1: λ，R，W三项之间有什么相互作用？</strong><br>&emsp;&emsp;答案：实际上这主要是为了让这个弯弯的曲线成为一条值值的绿线，你可以想像，也许你正在做一个回归问题，根据不同的多项式基函数，如果你加入这个回归惩罚项，也许模型确实非常逼近高幂次多项式函数，但是通过加入这个回归项，如果和数据拟合的很好，或者相对好，你就可以让模型的幂次数降低，所以你可以想像有两种方法可以做到这一点，一种是限制你的模型，这个模型在于不要更高的阶数或模型太过复杂，另一种就是，加入这个软性惩罚项，这样一来，模型依然可以逼近复杂模型的效果，比如像这个例子中的高幂次多项式，如果你想使用这些更复杂的模型，你需要克服这个惩罚，使用它们的复杂性，这就是这个关系，不太线性的分类。<br>&emsp;&emsp;实际上有很多不同类型的正则项，最常见的可能是L2正则项，但是还有很多你可能会看到的，这个L2正则项就是欧氏范数，其理念实际上是对权值向量的欧氏范数进行惩罚。有时候也会看到L1的正则项，是对权值向量的L1范数进行惩罚,将W中所有元素的值加和。L1正则项具有一些很好的性质，例如可以让矩阵W变得稀疏。还有就是弹性网络正则化，这是L1和L2的组合。还有max norm正则化，惩罚的max norm不是L1或者L2范数。这些正则化是你看到的不仅仅是应用在深度学习，同时也在机器学习的的很多领域，甚至更广泛地进行优化都有应用。在后面的课程中，也会看到一些更具体的正则化类型，例如Dropout，batch normalization。正则化的主要作用是为了减轻模型的复杂度，而不是去试图拟合数据。  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/p0Kaxyu.png" alt=""><div align="left"><br>&emsp;&emsp;其中，N是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数\lambda来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。</div></div></p>
<p><div align="center"><img src="https://i.imgur.com/pRlO7Gw.png" alt=""><div align="left"><br>&emsp;&emsp;这里有一些训练样本x，有两个不同的W正在考虑，这里x是一个包含四个1的4维向量。对于w我们来考虑两种不同的可能性。第一种情形是W的第一个元素是1，另外三个元素都是0，另外一种情形是W的全部元素都是0.25。当进行线性分类的时候，我们真正讨论的是x与W的点积结果，这时候两种情形的W其实都是一样的，因为和x点积的结果是一样的。现在可以考虑一下那种情形下加入L2后的线性回归模型会更好。当然是每个元素都是0.25的W的会更好，因为它具有较小的范数。在线性分类器中，W所反映的是x向量的值在多大程度上对输出有影响，所以L2正则化的作用是，它更加能传递出x中不同元素值的影响，它的robust可能会更好，更多的考虑了W的整体分布，令所有的元素具有较小的复杂度。L1正则化则具有完全相反的解释，当使用L1正则化的时候，我们更倾向于第一个元素为1，其他为0的W，L1正则化对复杂度具有不同的概念，即那个模型可能有较小的复杂度，在权值向量中，用数字0来描述模型的复杂度，L1更加喜欢稀疏解，倾向让大部分W的元素接近于0。如何度量复杂度是视不同问题而定，针对模型和数据。</div></div></p>
<p><div align="center"><img src="https://i.imgur.com/oyWU6c6.png" alt=""><div align="left"><br><strong>正则化的宏观理念就是：你对你模型做的任何事情，也就是种种所谓的惩罚，主要目的是为了减轻模型的复杂度，而不是去试图，拟合数据。</strong><br>&emsp;&emsp;需要注意的是，和权重不同，偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此通常只对权重W正则化，而不正则化偏差b。在实际操作中，可发现这一操作的影响可忽略不计。最后，因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当W=0的特殊情况下，才能得到损失值为0。<br>&emsp;&emsp;另外一个非常流行的选择是除了多类别SVM损失函数之外,深度学习中另外一个常用的选择是多项逻辑回归或者叫softmax。这个损失函数在深度学习中使用得更为广泛。当我们进行分类时，模型函数F输出了10个数字，这些数字代表了这些类别的得分，在多类型SVM中我们并没有针对类别的得分作出解释，我们只是希望正确分类的分数要比不正确分类的分数要高才好。但是对于softmax函数，我们将赋予这些分数一些额外的含义，并且会利用这些按分数针对我们的类别去计算概率分布。在这个函数中我们将其指数化令到结果都为正，然后用将指数求和来进行归一化，所以分数经过softmax函数处理后就可以得到概率分布。对所有的类别都有了对应的在0-1之间的概率，所有类别的概率和为1。我们将得到的概率值与目标值进行比较。如果真实的类别是喵，目标的概率分布就应把所有的概率击中到喵上，所以喵的概率是1而其他类别是0。我们现在要做的事情是将softmax计算的结果去匹配上述的目标概率分布。这里我们的损失函数就是正确类别概率的对数再取负值，我们希望概率接近1。同时log函数式一个单调函数，当针对正确类别的最大化 logP ，意味着想要它变高，但是损失函数式用来度量不好的程度，所以对其取负值将求最大值变为求最小值。  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/pGSMQ5d.png" alt=""><div align="left"><br>在Softmax分类器中，函数映射:  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/UevpOeU.png" alt=""><div align="left"><br>保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将折叶损失（hinge loss）替换为交叉熵损失（cross-entropy loss）。公式如下：  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/31PTdQm.png" alt=""><div align="left"><br>在上式中，使用f_ j来表示分类评分向量f中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值L_i的均值与正则化损失R(W)之和。其中函数:  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/RgzM3kd.png" alt=""><div align="left"><br>被称作softmax函数：其输入值是一个向量，向量中元素为任意实数的评分值（z中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。<br><strong>概率论解释：</strong>先看下面的公式：  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/B1iank7.png" alt=""><div align="left"><br>可以解释为是给定图像数据X_i，以W为参数，分配给正确分类标签y _i的归一化概率。为了理解这点，请回忆一下Softmax分类器将输出向量f中的评分值解释为没有归一化的对数概率。那么以这些数值做指数函数的幂就得到了没有归一化的概率，而除法操作则对数据进行了归一化处理，使得这些概率的和为1。从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE）。该解释的另一个好处是，损失函数中的正则化部分R(W)可以被看做是权重矩阵W的高斯先验，这里进行的是最大后验估计（MAP）而不是最大似然估计。提及这些解释只是为了让读者形成直观的印象，具体细节就超过本课程范围了。<br><strong>实操事项：数值稳定。</strong>编程实现softmax函数计算的时候，中间项因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数C，并把它变换到求和之中，就能得到一个从数学上等价的公式：  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/AFNan84.png" alt=""><div align="left"><br><strong>让人迷惑的命名规则：</strong>精确地说，SVM分类器使用的是折叶损失（hinge loss），有时候又被称为最大边界损失（max-margin loss）。Softmax分类器使用的是交叉熵损失（corss-entropy loss）。Softmax分类器的命名是从softmax函数那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。<br><strong>Softmax Classifier的计算规则如下：</strong></div></div></p>
<p><div align="center"><img src="https://i.imgur.com/m43tHjz.png" alt=""><div align="left"><br>&emsp;&emsp;将线性分类器得到的分数，先将所有的分数指数化以便得到的结果都是正数，接着我们利用这些指数的和来归一化它们，当我们的分数经过softmax函数处理后，我们得到了概率分布，对于所有的类别我们都有了相应地概率，每个概率都介于0和1之间，所有类别概率的和为1。这正是我们期待的解释，这是结算出来的概率分布，他是从分数推导出来的，我们拿它同我们的目标值，或者说真实的概率分布进行比较，所以如果我们知道那个东西是一只猫时，目标的概率分布，应该把所有的概率集中在猫上面，所以我们可以得到猫的概率是1，而对于其他类别而言概率是0。现在我们需要做的就是促使，我们计算得到的概率分布，就是通过那个softmax计算的结果，去匹配上诉的目标概率分布，即正确的类别应该是具有几乎所有的概率。我们这么做的方法，你可以用多种方式来列这个方程，你可以在目标概率分布与计算出的概率分布间，计算KL散度，以比较他们的差异，你可以去做一个最大似然估计，但是现在我们想要的是真实类别的概率应该比较高并接近于1，然后我们的损失函数就是，真实类别概率的对数再取负值，这可能有点难以理解，因为我们是用不同的东西构建出来的，你只需要记住的是我们希望的概率，趋近于1，所以log函数是一个单调函数它遵循数学上规定的走势，找log函数的最大值相对简单，只要我们使概率最大就可以了，所以我们选择了log函数，由于log函数单调，当我们针对正确类别最大化logP，意味着我们想要它变高，但是损失函数是用来度量坏的程度，不是好的程度，所以我们添加了一个负号，让它更符合我们的预设，所以针对正确的类别，softmax的损失函数即将会变成-logP。<br>&emsp;&emsp;<strong>小结：</strong>我们用softmax对分数进行转化处理，并得到<strong>正确类别</strong>的损失函数是 <strong>-log P</strong>。  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/CCZsqFI.png" alt=""><div align="left">  </div></div></p>
<p>&emsp;&emsp;<strong>Q:What is the min/max possible loss L_i ?</strong><br>答案：最大值是无穷大，最小值是0。针对正确的类别，我们希望的概率是1，不正确的类别的概率是0。如果是这样的话，那么log函数离得自变量就会是1，因为那是真实类别所对应的概率，1的对数是0,1的对数的负值仍然是0，这意味着如果我们所有事情做对了，我们的损失函数会是0，为了保证我们所有的事情都做对了，我们的分数应该是什么样的呢？ 所以分数实际上必须相当的极端，趋于无穷远，因为我们进行了指数化，归一化，我们真正可以得到一个1和0的概率分布，唯一的方法就是，为正确分类取一个无穷大的分值，然后为所有不正确的分类，取负无穷大的分值，电脑在无穷大方面做得并不好，所以在实践中，你永远不会得到零损失，在这个东西有限的精度，但你仍然有这个解释，这里零是理论上的最小损失，最大损失是无限的，所以假设我们正确分类的概率质量函数为0，那么你会对其进行求对数之后取负值，0的对数是无穷大，那么取负值之后0的对数是正无穷大，所以这真的很糟糕，但是再强调一次，你永远不会遇到这种情况，因为你唯一能达到这个概率是0的情况是，是e的正确分类值次方是0，而这只有在正确分类值是负无穷时才会发生，所以，再次强调你永远不会达到这个有限精度的极值。  </p>
<p><div align="center"><img src="https://i.imgur.com/Z4jb9FA.png" alt=""><div align="left"><br>&emsp;&emsp;<strong>Q2:Usually at initialization W is small so all s ≈ 0. What is the loss ?</strong><br>答案：如果所有的S都很小，近乎为0，那么损失值是log(C),C为类别数，如果你正在用这个softmax损失来训练一个模型，你应该在第一次迭代中检查，如果他不是logC，那么出了问题。<br>&emsp;&emsp;<strong>那么我们可以对比这两个损失函数</strong>  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/OieVitL.png" alt=""><div align="left"><br>&emsp;&emsp;在线性分类方面，这个设置看起来一样，我们将W和输入向量相乘，得到分值向量，那么这两个损失函数的区别是，我们如何来解释这些分值，进而量化度量到底有多坏，对于SVM深入研究并观察正确分类的分值和不正确分类的分值的边际，而对于这个softmax或交叉熵损失，我们打算去计算一个概率分布，然后查看负对数的概率的正确分类，所以有时候如果你看。</div></div></p>
<p><div align="center"><img src="https://i.imgur.com/0IzqZC7.png" alt=""><div align="left"><br><strong>Q3:假设我有这个例子，如果你改变它的分数，所以假设我们有三个分数，忽略底部部分，损失函数会怎么变化？</strong><br>&emsp;&emsp;答案：但请记住，如果我们回到这个例子，在多分类SVM损失函数中，当样本中有车，那么车的分值就会比其他不正确的分值要高，即使车的得分值稍微有所变化，也并不会对结果产生根本改变，因为对SVM损失函数来说，唯一关心的是正确的分值要比不正确的分值高出一个安全边际，但是softmax这种方式就相当不同了，softmax的目标是将概率质量函数（离散分布值）等于1，所以你给正确分类再高的分值，同时给不正确分类再低的分值，softmax依然会在正确分类上，积累更多的概率质量，正确的阶级向无穷大迈进，并持续将这一分值推向无穷大，并将不讲正确分类的分值推向负无穷，在实际应用中这两个损失函数之间的差别到是很有趣，那SVM它会得到这个数据点超过阈值，要正确分类，然后放弃，它不再关心那个数据点，而softmax只是总是试图不断提高，每个数据点都会越来越好，所以这是一个有趣的差异。  </div></div></p>
<p><div align="center"><img src="https://i.imgur.com/2jYJJco.png" alt=""><div align="left"><br>&emsp;&emsp;<strong>要重塑我们从这里来的地方，是我们有一些xs和ys的数据集，我们使用我们的线性分类器来获得一些分数函数，根据我们的输入x计算我们的分数S，然后我们将使用损失函数，也许softmax或SVM或其他损失函数，来定量的计算我们的预测有多糟糕，y这个真实的目标，然后我们会经常增加这个损失函数，一个正则化的术语，试图在训练数据之间权衡，并喜欢更简单的模型，所以这是一个非常通用的概述，很多我们称之为监督学习的东西。</strong><br><strong>内容到此，大家加油</strong></div></div></p>

    </div>

    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="" target="_blank">Snippet</a>
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
    
        <a href="/2018/04/25/图像分类笔记cs231n2/" class="next-post btn btn-default" title='图像分类 *线性分类I*'>
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">图像分类 *线性分类I*</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>


    <div id="comments">
        
	<script type="text/javascript" src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
    var gitment = new Gitment({
        id: window.location.pathname,
        owner:"",
        repo:"",
        oauth: {
          client_id:"",
          client_secret:""
        },
        perPage:"10",
    });
    gitment.render('comments');
</script>

    </div>





                </main>
                
    <aside class="col-md-4 sidebar">
        
        
    <div class="widget">    
        <h3 class="title">搜索</h3>
        <div id="search-form">
            <div id="result-mask" class="hide"></div>
            <div class="search-area">
                
                    <input id="search-key" type="search" autocomplete="off" placeholder="搜点什么呢?">
                    <button type="button" class="search-form-submit" id="search-local">站内搜索</button>
                
                
                    <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Google Search"><button type="submit" class="search-form-submit">谷歌搜索</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
                
            </div>
            <div id="result-wrap" class="hide">
                <div id="search-result"></div>
            </div>
            <div class="hide">
                <template id="search-tpl">
                    <div class="item">
                        <a href="/{path}" title="{title}">
                            <div class="title">{title}</div>
                            <div class="content">{content}</div>
                        </a>
                    </div>
                </template>
            </div>
        </div>
    </div>

        
        
    <div class="widget notification">
        <h3 class="title">网站公告</h3>
        <div>
            <p>昨天的你的现在的未来<br/>
<p>Never forget why you started, and your mission can be accomplished<br/>
<p>邮箱：wulehuihnu@163.com 欢迎<br/> 
        </div>
    </div>

        
        
    <div class="widget">
      <h3 class="title">社交</h3> 
        <div class="content social">
            
	            <a href="//github.com/lehuiwu" rel="external nofollow" title="Github" target="_blank">
			    	<i class="git fa fa-git"></i>
			    </a>
            
	            <a href="//mail.163.com" rel="external nofollow" title="邮箱" target="_blank">
			    	<i class="envelope-o fa fa-envelope-o"></i>
			    </a>
            
	            <a href="/" rel="external nofollow" title="联系QQ" target="_blank">
			    	<i class="qq fa fa-qq"></i>
			    </a>
            
	            <a href="/" rel="external nofollow" title="微博" target="_blank">
			    	<i class="weibo fa fa-weibo"></i>
			    </a>
            
	            <a href="/" rel="external nofollow" title="QQ群" target="_blank">
			    	<i class="users fa fa-users"></i>
			    </a>
            
	            <a href="/atom.xml" rel="external nofollow" title="RSS" target="_blank">
			    	<i class="feed fa fa-feed"></i>
			    </a>
            
        </div>
    </div>


        
        
    <div class="widget">
        <h3 class="title">分类</h3>
        <ul class="category-list"><li class="category-list-item"><a class="category-list-link current" href="/categories/深度学习/"><i class="fa" aria-hidden="true">深度学习</i></a><span class="category-list-count">5</span></li></ul>
    </div>


        
        
    <div class="widget">
      <h3 class="title">归档</h3>
        <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/"><i class="fa" aria-hidden="true">四月 2018</i></a><span class="archive-list-count">5</span></li></ul>
    </div>


        
        
  <div class="widget">
    <h3 class="title">标签云</h3>
    <div class="content tag-cloud">
        <a href="/tags/caffe/" style="font-size: 10px;">caffe</a> <a href="/tags/caffe-matlab/" style="font-size: 10px;">caffe matlab</a> <a href="/tags/cs231n/" style="font-size: 20px;">cs231n</a>
    </div>
  </div>


        
        
    <div class="widget">
        <h3 class="title">友链</h3>
        <div class="content friends-link">
        
            <a href="https://my.csdn.net/" class="fa" target="_blank">CSDN博客</a>
        
        </div>
    </div>


        
    </aside>

            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="hide">
	<i class="fa fa-chevron-up"></i>
</a>




    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12"> 
                <span>Copyright &copy; 2018
                </span> | 
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> | 
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>



	<script src="/js/search.js?rev=@@hash"></script>


<script src="/js/app.js?rev=@@hash"></script>


</body>
</html>